Authors	Year	Publisher	Citations	DOI	Type of research	Subfield	(L)LM used	Research questions	Methodology	Relevant citations	Critical summary of use of LLMs							
Philip Huebner, Eliot Sulem, Cynthia Fisher, Dan Roth	2021	Association for Computational Linguistics	74	https://doi.org/10.18653/v1%2F2021.conll-1.49	Experimental	Language acquisition	BERT	"1) Can we use insights from work in language acquisition to build more efficient - and better performing - models? 2) how quickly and to what level can a TLM acquire basic English grammatical phenomena when provided with input matched in size to that of the average English-speaking 6-
year-old? 3) does child-directed language data
play a special role in grammar induction compared
to conventional written text data, like Wikipedia
articles? "	Scaled-down masked language model based on RoBERTa = BabyBERTa. Trained on AO-CHILDES + 3 Wikipedia corpora. Grammar test = adaptation of BLiMP.	"1) ""Although TLMs were developed for applications in language technology, their successes raise
fundamental questions for acquisition research, including unsupervised grammar induction."" (p.624). 2) ""Collectively, these results suggest that it is possible to acquire grammatical knowledge from substantially less data. However, it is not yet known if this extends to child-directed speech, which is the topic of this work."" (p. 625) 3) ""The direction of this effect is what one would predict under the assumption that input to children aged 1-6 years but not beyond (6-12 years) scaffolds grammatical development"" (p.630). 4) ""from a cognitive plausibility perspective, holistic scoring resembles much more closely the actual situation faced by humans tasked to judge grammatical acceptability. ""(p.631). 5) ""claims about what children might learn without the aid of built-in linguistic knowledge should be based on models trained on developmentally plausible datasets. "" (p.632)"	"The authors use a fine-tuned version of RoBERTa, a fine-tuned model of BERT (Google, October, 2018) to investigate the relationship between human language acquisition and Transformer model acquisition. The investigation is bidirectional: it investigates how language acquisition theories can inform us with how to train the model to improve performance (RQ1 and 2) and how the model can inform us on how children acquire language (RQ2 and 3), or better what is learnable by children. The secon direction is given implicitly in the research questions but it is uncovered throughout the article (see citations n. 4 and 5 as an example). The study aims at providing a model that is ""developmentally plausible"" (p. 624 and 632) to investigate language acquisition, but also at finding more efficient training solutions for a more feasable training and equal (or better) performance."							
Alex Warstadt and Samuel Bowman	2024	Taylor & Francis Group	69	https://doi.org/10.48550/arXiv.2208.07998	Review	Language acquisition	Transformers and LSTM	What artificial neural networks can teach us about human language learning.	Article review and summary	1) If the model succeeds after ablating A, it provides a proof of concept that the target is learnable without A. If the model furthermore does not enjoy any substantive advantages over humans, then we can conclude the result is likely to generalize to humans, and considerations from learnability do not justify the claim that humans require A. (p. 3-4) 2) The benefit of studying a model is to generalize results from a tractable or observable setting to a more fundamentally interesting setting. Thus, the most useful learnability results from model learners are ones that are likely to imply similar conclusions about humans. (p.5) 3) Even imperfect models can provide useful evidence about human language learning. (p.6) 4) The reason why positive results are more generalizable is simply that it is easier to build models that undershoot the advantages of humans. (p.7) 5) positive results or proofs of concepts are more practically generalizable than negative results. (p.8) 6) We can also study performance to make inferences about competence. We can construe performance very broadly to include many aspects of behavior, ranging from acceptability judgments to order of acquisition and reading time. Although this has its limitations—two systems that have identical behavior in some respects could have very different internal functioning—the more behavioral similarities we observe between two systems, the greater the evidence that they share an underlying mechanism. 7) supervised tasks can be constructed much like artificial language learning experiments on humans. 8) Dupre (2021) discusses the relation between ANNs and competence at length and suggests that ANNs are better viewed as models of human performance rather than competence because they are optimized to reproduce the output of human performance. We broadly agree with this view, and note that it does not contradict our claim that competence for ANNs may still be well-defined and testable. (p. 9, footnote) 9) Language model scores can be used to predict other aspects of human linguistic performance. (p. 11) 10) Predicting age-of-acquisition is another possible point of comparison between humans and models. (p.11) 11) Although the input to humans is not annotated with linguistic features, training and testing models on a supervised task with such labeled data can still provide useful evidence about human learning (...)  the experiment can tell us whether an inductive bias, such as a hierarchical bias or a compositionality bias, can be acquired through exposure to the unstructured learning environment. (p. 14) 12)  model learners will be able to prove that specific linguistic behaviors are learnable under impoverished conditions, and thereby help to establish the causal roles of hypothesized advantages in the learning environment and the learner. (p. 26) 13) Machine learning and NLP are advancing at an unprecedented rate. This makes the prospect of using artificial learners as models of human language acquisition an especially salient possibility (...) It is only natural that such a shift in our understanding of the learnability of language learning should have some real impact on debates about human language acquisition. (p. 27).	The authors make a case for LMs, as Transformers and LSTM models, to be able to redefine our understanding of human language acquisition. Learnability research is systematically reviewed to show the challanges and potentials of this research direction, arguing that, despite the deep differences between these models and humans, they can still generalize to human language development both from a competence and performance perspective. The condition for this generalization is the comparability of the learning enviroment and pre-conditions to which the learner, either human or artificial, is exposed. By reducing the differences in the pre-conditions we can more confidently generalize to humans about underlying mechanisms of language acquisition. Thus, the paper argues for an understanding of these models as useful explanations for human linguistic behavior - language acquisition in this case. However, it seems that language acquisition is reduced to the study of what it is possible to learn and by observing what the models learn from pre-defined conditions we can infer that humans can also learn that given those pre-defined conditions. However, this line of research seems mostly useful to investigate properties of the input rather than properties of the learner. In line with research in learnalibility, the models yields proofs of concepts of what is theoretically learnable given certain constraints, but it is not clear from the paper how they would inform us on underlying mechanisms of leraner's learning process. Thus, this unclarity may lead one to see these models, best case scenario, as adding nothing more than what it can be studied by computational learning theory (CLT) (Gold, 1967) without employing generative AI (compare this in the discussion with the article of CNBC about burnout of AI employees). This is because the learnability is assessed by testing what kind of output the model generates instead of computing methamatically what it is feasibly learnable. On the other hand, if we consider these models some kind of a replica of the actual human linguistic cognition, there it is possible to observe the added value and the reason why an acquisitionist might want to spend their research time in fine-tuning models, computing a comparable inductive bias and so on and so forth (see pp. 14-23 for a list of problems of these models). The value of this kind of research is recognizable only under the assumption that these models are better explanations of human linguistic cognition and, therefore, can replicate it so accurately to justify the choice of testing them using tasks that are not applicable to human participants (see p. 27). Finally, it seems that the arguments which the models may support are fundamentally aimed at questioning the poverty of stimulus, thus directed to one specific hypothesis in the broader landscape of language acquisition. Thus, it may be more appropriate to explicitly claim that the models may be used to support theories that are in opposition to chomskian approaches, as Piantadosi (2023) does.							
R. Thomas McCoy, Robert Franz and Tal Linzen	2018	Annual Meeting of the Cognitive Science Society	85	https://doi.org/10.48550/arXiv.1802.09091	Experimental	Language acquisition	LMs trained on 66 English words. Encoder-decoder RNN. GRU = recurrent unit compared to humans	Is a structure-sensitivity constraint necessary to account for the generalizations that human language learners make? Specifically, they test the hierarchical generalization of subject-auxiliary inversion in RNNs.	RNN trained on two languages: 1) no-agreement language, 2) agreeement language. Model's task: identity and question formation. The model had to reproduce the input sentence (identity) and then form the question of the input sentence (question formation). Comparison of the output of the only model that learnt the pattern to two types of error found in Crain and Nakayama (1987).	"1) of the six RNN architectures we explored, one of the architectures consistently learned a hierarchical generalization for question formation. This suggests that a learner’s preference for hierarchy may arise from the
hierarchical properties of the input, coupled with biases implicit in the network’s computational architecture and learning procedure, without the need for pre-existing hierarchical constraints in the learner. We provide further evidence for the role of the hierarchical properties of the input by showing that adding syntactic agreement to the input increased the probability that a network would make hierarchical generalizations, 2) We now return to the full questions produced by our networks and compare the networks’ errors to the types of errors that humans make when acquiring English, 3) This error type is common among English-learning children (Crain & Nakayama, 1987) and is compatible with hierarchical generalization. (par. Comparing RNN Mistakes with Human Mistakes) 4) Such errors were never observed by Crain and Nakayama (1987) and are incompatible with a hierarchical generalization. (par. Comparing RNN Mistakes with Human Mistakes). 5)  though the networks’ common error types overlapped with the common error types for humans, the networks also frequently made some mistakes that humans never would. (par. Comparing RNN Mistakes with Human Mistakes). 6) We have investigated whether a learner without such a constraint can learn the hierarchical generalization without the critical disambiguating examples. Based on the behavior of one of the architectures we examined (GRU with attention), the answer to this question appears to be yes. The hierarchical behavior of this non-hierarchically-constrained architecture plausibly arose from the influence of hierarchical cues in the input (par. Conclusions and Future Work). 7) It is not clear why it was the only architecture to do so; we intend to examine the differences in behavior between the recurrent units in future work. (par. Conclusions and Future Work) 8) l languages with a corpus of child-directed speech. Second, even if our findings do generalize to realistic language, we would only be able to conclude that it is possible to solve the task without a hierarchical constraint; humans certainly could have such an innate constraint despite it being unnecessary for this particular task(par. Conclusions and Future Work)"	The authors use RNNs to find evidence that it is possible to learn hierarchical constraints without assuming innatism. Thus, the models are used as a proof of concept that the poverty of stimulus does not justify the hypothesis of innateness of language in humans. Thus, the possibility evidenced by the behavior of one RNNs does not pertain to the mechanisms involved in language acquisition but simply tests the theoretical validity of the poverty of the stimulus. The authors aknowledge the fact that this approach cannot refuse innatism in humans, therefore it is unclear what is the added value of using a LMs for claims about human language acquisition. Moreover, the source of evidence for what it is in principle possible to learn comes from an artificial output, which behaves only partially in a human-like manner, as it is pointed out. Thus, without assuming some degree of identification between the model and human linguistic cognition it is unclear what led the authors to employ these models for such an inquiry.							
Goldstein et.al. (10 authors from google) 	2021	bioRxiv	44	https://doi.org/10.1101/2020.12.02.403477	Experimental	Neurobiology of language	GPT2	" 1) whether the brain, like autoregressive DLMs, predicts upcoming words while listening to natural speech (how good are humans at next-word
prediction? How much do human predictions align with DLM predictions?)                    2) whether the brain, like autoregressive DLMs, encodes the unique, context-specific meaning of words based on the sequence of prior words."	Comparison between the brain activity in word prediction and the neural activity of GPT2. However, the first comparison was only behavioral: humans and GPT2 were asked to predict the next word explicitly as a first task.	"1) Together, our findings provide compelling evidence for shared core computational principles, of prediction and
contextual representation, between autoregressive DLMs and the human brain, and support a new modeling framework for studying the neural basis of the human language faculty. (p.3) 2) Human predictability scores and GPT2 estimations of predictability were highly correlated. This suggests that GPT2’s and humans’ next-word predictions are similar in natural contexts. (p.4) 3) it demonstrates how autoregressive DLMs’ behavior can be used for modeling humans’ predictions at behavioral and neural levels (p. 11), 4) Deep language models (DLMs) provide a new modeling framework that drastically departs from classical psycholinguistic models. 5)  The current paper provides compelling behavioral and neural evidence for deep connections between autoregressive DLMs and the human brain (p.16) 6) Can DLMs, such as GPT2, provide insights into the cognitive mechanisms underpinning the human language faculty. We hypothesize that the family of DLMs is sharing certain critical computational principles with biological language. This does not imply that they are identical, nor that they share the same circuit architecture. Human brains and DLMs share computational principles but they are likely to implement them using radically different neural architectures (p.17)"	"The authors investigate the internal states of GPT2 and the brain activity of human participants to make connections between the model's behavior about word prediction and human behavior. The study aims at showing the validity of LLMs as cognitive models, because they show deep similarities in handling language and may be considered as ""biologically feasibile computational frameworks for studying the neural basis of language"" (abstract). However, the formulations used in the conclusions are vague and rather inclunclusive, as it is not clear what it has to be taken home as a core message of the study. After describing all these shared properties between LLMs and the human brain, they highlight how LLMs should not be identified with the brain itself and, even if they show similarities at the computational level, they implement the computations in very different ways (humans most likely don't have neural networks architecture as GPT2). Moreover, they point out how the models are performance-oriented and ""deemphazise interpretability"", which should make them very bad cognitive models, following this claim. Therefore, all these final remarks make the goal of the study rather ambiguous, if the reader does not assume that the authors are trying to demonstrate that LLMs are replicas of human linguistic cognition that researchers can use as a source of evidence for neurolinguistic theories."							
Dota Tianai Dong and Mariya Toneva	2023	Under review	Under review	https://doi.org/10.48550/arXiv.2311.07766	Experimental	Neorobiology of language	MERLOT Reserve. Multimodal video transformer.	1) Do models learn any brain-relevant shared information between individual modalities, such that the joint representations are better predictors of brain activity than the ablated representations?2) Do models learn any brain-relevant new information when individual modalities interact, such that the joint representations are better predictors of brain activity than the sum of the ablated representations?	"fMRI datsets of 5 subjects watching fragments of TV show Friends compared to the network activity in each layer. The comparison is between the neural activity in the layers of the model and the brain regions involved in ""language-only"" and multimodal language. A specific comparison is between the angular gyrus and the bottom layers of the model which seem to align."	" 1) we turn to the only system that we have that truly integrates complex visual and complex language information–
the human brain–to improve our understanding of vision-language interactions and integration in a popular multi-modal video transformer. (p.1) 2) We expect that a model that is able to learn how to connect and integrate vision and language modalities in a brain-relevant way would significantly align with these [language related] regions [in the human brain]. (p. 2) 3) our second key insight is to contrast the brain alignment of the joint vision-language
model internal states with the brain alignment of internal states obtained from the same model but under different conditions, which have been carefully designed to reveal whether the model connects and integrates multimodal information in a brain-relevant way. (p.2) 4) we show evidence that one popular multimodal video transformer partially aligns with vision-language integration in the brain. (iii) We demonstrate that vision can contribute to language processing in the brain, largely due to masked language modeling. (p.3) 5) Through the lens of the human brain, our work makes a contribution to this area by providing a human-derived reference that captures complex multimodal connections and interactions. (p. 3) 6) Do current multimodal video models learn brain-relevant cross-modal connections between individual modalities? As our main focus is on understanding the influence of vision on language, we compare brain alignment between vision-language and language-only representations in the language regions. (...) We find that incorporating the inputs from the vision modality significantly improves brain alignment
over language regions (p.5) 7) We further observe that the contrast of brain alignment mostly peaks in the later layers (9-11) of the model, suggesting that the later layers of these models encode the most brain-related properties of video stimuli (p.6) 8) Our findings suggest that incorporating visual inputs may enhance the model’s capacity through cross-modal connections between vision and language, possibly akin to the convergence
of vision and language representations observed in the angular gyrus region. (p.6) 9) We suspect that the benefits of vision we identified by the ablation of vision information in the current models are far from fully encompassing the entire spectrum of multimodal integration processes taking place in the brain. (p.6) 10) It also offers evidence that audio-visual correspondences could be an important factor shaping the role of the angular gyrus as a cross-modal hub for the convergence of multisensory information (p.7) 11) Recent work has shown that multimodal image-caption models can better explain high-level vision semantics than unimodal models (Wang et al., 2022; Reddy Oota et al., 2022). We contribute to this line of work in three ways: 1) we show the effects of visual information on language processing. 2) we extend the setting to fully multimodal brain recordings. 3) We employ multimodal video transformers, demonstrating their potential as a valuable resource for studying brain representations of video stimuli. 12)We situate our work at the intersection of neuroscience and machine learning, with implications
for both fields. The implications for machine learning: We show to what extent the models
have learned brain-relevant cross-modal connections through the prediction of masked tokens. We
provide novel evidence that the cross-modal connections can benefit individual modalities. We
identify that current models fall short of capturing multimodal interactions, using the brain as a
test bed. We propose a promising and sufficient approach for improvement: fine-tuning a task that
requires inference between language and vision. (p.9)."	"The authors use a LMs trained on multimodal data to contribute to two research fields: they contribute to machine learning by finding evidence of improved performance (i.e. more human-like) by integrating both vision and text in the training input; on the other hand, they claim to contribute to the neuroscience research, by investigating the role of vision in language. Thus, they use the model as a source of evidence for generalizations over human language. In this case, they draw clear connections between the internal state of the network and specific parts of the brain involved in processing multimodal input (ex. angular gyrus). Therefore, this is a clear example of research overlapping the model with human language. Even though it is valuable the use of careful terms such as ""brain-relevant ways"", it is still observable how this terminology still represents a confusion of the model with the actual phenomenon, inducing the reader to conclude that the models recruit human mechanisms to carry out their tasks and for this reason can be used as source of evidence for linguistic theories of human language."							
Tal Linzen and Brian Leonard	2018	"
Annual Meeting of the Cognitive Science Society"	55	https://doi.org/10.48550/arXiv.1807.06882	Experimental	Syntax and cognition	RNNs with a single layer of LSTM	To what extent the detailed pattern of errors made by RNNs matches the errors made by humans. Do RNNs match the humans errors affected by attraction, number asymmetry, relative clause advantage and cumulative attraction?	Experiment 1 with humans compared to simulation with RNN of experiment 1. Experiment 2 with humans and simulation with RNN. Experiment 1: reading task, 3 conditions. RSVP paradigm, SPR paradigm, Untimed paradigm. Errors patterns of attraction, number asymmetry and relative clause advantage are measured. Sumulation experiment 1: RNN supervised for word prediction (predict the number of the upcoming verb). Trained on English Wikipedia corpus (raw textual input: unlebelled). The model is used as a language model. Experiment 2: reading task, only SPR paradigm. Measure of error pattern of cumulative attraction. Same thing tested for the RNN simulation.	"
1) While the RNNs’ degraded performance on sentences with attractors suggests that syntactic processing in RNNs is imperfect, it is not immediately clear that they differ from humans in this respect. 2) This raises the possibility that the syntactic representations that emerge in RNNs are similar to those used by humans to process language. 3) Two aspects of the networks’ error patterns are consistent with the human data: first, agreement errors were more common when the local noun did not match the subject in number; and second, these attraction errors were more likely when the subject was singular and the local noun plural than the other way around. Unlike humans, errors were much more likely when the attractor was embedded inside a relative clause. 4) In other words, in RC-first sentences RNNs were similar to humans, but for the wrong reason: those sentences confounded proximity to the subject (the reason for human errors) with the presence of an attractor inside an RC modifier 5) These findings suggest that the syntactic representations acquired by RNNs differ from those used by humans in sentence processing. "	The authors use a language model, referred to as a recurrent neural network to investigate to what extent the model behaves human-like when parsing sentences. Specifically, they investigate to what extent the model matches the typology of mistakes humans make when exposed to sentences that require knowledge of syntactic structure. Thus, the authors draw a direct connection between the model and the human language in the attempt to investigate how much humanness the model displays. They evidently compare the models to humans as they were comparing two groups of participants in an experiment, using human subjects as a crontrol group and the RNNs as the experimental group. Employing such a methodology to investigate which kind of hierarchial knowledge the model has, shows a clear approach to the model as a replica of human linguistic cognition. The importance of recognizing this assumption lies on the fact that it is again not clear what the relevance of the study should be, other than assessing some aspects of the model's output and reporting that, based on its output only, it does not display the same hierarchical representations that humans have. Thus, the research question as it is posed suggests an understanding of the model as a potential replica of humans' cognition and after finding results that do not support this (implicit) hypothesis it is not clear what the relevance of the study is, rather than debunking beliefs that are maybe implicitly shared in some research communities using (L)LMs. Testing the model itself as it was a human participant is already ambiguous as it needs a starting assumption that 1) these models are at least how-actually models 2) these models are a replica of what is going on in humans' brains when parsing sentences. Finally, in line with this initial ambiguity, the discussion session limits to summarize the results without any elaboration that would help the reader to understand what these findings mean for linguistics and/or for machine learning - it is more easily understandable what can be the contributions for machine learning. Are we gaining more insights into how humans parse sentences and handle compex syntactic structures? Are these models good representation of human syntactic representations and more broadly human linguistic cognition?							
Shammur Absar Chowdhury and Roberto Zamparelli	2018	Association for Computational Linguistics	74	https://aclanthology.org/C18-1012	Experimental	Syntax and cognition	LSTM and GPU RNN	1) Are (R)NN feasible models of innate-grammarfree language learners? 2) Which abstract properties can they learn from the input? 	"LSTM and GPU RNN trained on wikipedia corpus. The model is used as a classifier. Trained on raw data (unlebelled). Model tested on 3 tasks: A) subject vs object relative clause; B) WH- extractions; C) Subject and relative islands violations. No direct comparison with human subjects. The results of the model's output are compared to the psycholinguistic litarature about grammaticality judgements tasks in humans about these three same tasks employed in the simulation."	"1) These advances raise the question whether similar models, trained on corpora of naturally occurring sentences, could come to approximate the full range of human grammaticality judgments, including judgments on structures which, unlike agreement, are virtually non-existent in the training input. The ability to do so would have implications for the debate on language innatism (p.134) 2) a generalist (R)NN has no language-specific learning skills, no innate linguistic parameters to set (Chomsky and Lasnik, 1993; Baker, 2001); if such a device could manage to replicate fine-grained human intuitions inducing them from the raw training input this would be evidence that exposure to language structures (albeit in an amount orders of magnitude larger than the one a child receives, and without a connection to the non-linguistic context of utterance) should in principle be sufficient to derive a syntactic competence, against the innatist hypothesis (...) Suppose on the other hand that NNs could approximate human intuitions on some linguistic phenomena but not on others, despite similar statistical distributions in the training input: this would now count as strong evidence that the ‘unlearnable’ phenomena tap on aspects of the grammar faculty that have limited representations in normal language samples, and are good candidates for being innate. (p.134) 3) The question is which NN measure best corresponds to the speaker’s perception of ungrammaticality in (1b), keeping into account that even in the theoretical and psycholinguistic literature there are no established metrics to measure ‘degrees of ungrammaticality’. (p.135) 4) So far we have seen that the RNN model was able to distinguish grammatical from ungrammatical pairs with some success, but also to capture a number
of interesting effect from the psycholinguistic literature (p.140) 5) Has our model learned syntactic islands from Wikipedia? It would appear so (...) (p.140) 6) This data shows that the increased perplexity with Wh cases has nothing to do with island effects (...) (p.141) 7) The results showed, first of all, that our NN model was good at capturing known effects in the processing of relatives (p.142) 8) On the other hand, the overwhelming effect of processing factors like the level of embedding (Figure 2), and the fact that the apparent success of the NN in the island task in not based on the island extraction effect itself cast doubts on the idea that the NN is using an abstract dimension of ‘grammaticality’. It could be tempting to take this as a cue that even human ungrammaticality should be reduced to processing (see Villata et al. 2016 for discussion in the domain of weak islands, which we did not test
here), but there are reasons to believe that, while processing might play a role, it cannot be the whole story. (p.142) 9) Only after these issues have been resolved and a performance plateau has been reached we will be in a position to go back to the original question: are (R)NN feasible models of innate-grammarfree language learners? Which abstract properties can they learn from the input?"	The authors run a simulation with RNN trained on language to compare it with psycholinguistics literature on three main syntactic structures that are commonly used in the literature to argue in favor of language innatism in humans. The purpose of this experiment is to argue against the hypothesis of innatism grounded on the theory of the poverty of stimulus. Their logical inference is builts as follows: if the model is able to perform human-like when prompted with tasks aimed at tapping knowledge of the three hierarchial stracture under investigation, then this is evidence that innatism is not a plausible explaination for human acquisition of those syntactic structures. More in general, it would be evidence that disproved innatism as a theory. On the other hand, if the model does not perform human-like (i.e. the model responds as a human in some cases and in other not) then this is strong evidence that innatism is a valid theory of language acquisition in humans. The logical inference follows modus ponens and modus tollens as it is intruduced at the beginning of the paper, however, the conclusions surprisingly violete this stipulation (see citation n. 9.) Thus, the authors follow already a rather flawed reasoning by using RNN'S output as evidence that generalizes over humans, but even when their own reasoning disproves their hypothesis they do not refuse it, but rather state that more research is needed. Specifically, the research that they claim is needed would inevitably end up proving their original statement as they regard as scientific evidence of internal syntactic representations (thus, underlying mechanisms) the model's behavior. Thus, if the model's behavior finally succeeds to align to the human behavior this would be proof of the non-existence of hypothesized mechanisms in humans' cognition (i.e. innatism). The logic is rather circular and essentially flawed, as RNNs might learn eventually to capture these structures with human intervetion that might actually recreate the starting conditions that innatism theory claims to be present in humans (therefore at best supporting it) and more fundementally it disregards the principle of multiple realizibility, for which comparing two superficial behaviors to make claims about inner mechanisms of one of the two does not make any sense.  							
Yair Lakretz , Stanislas Dehaene and Jean-Rémi King	2020	Entropy	29	doi:10.3390/e22040446	Theoretical review	Syntax and cognition	ANNs optimized for language modeling	"What Are the Computational Mechanisms that Explain Our Limited Ability to Process
Specific Sentences? A proposal of a new theoretical framework based on artificial neural networks."	Article and theory review and summary.	"1) We suggest here an alternative theoretical framework—taking the recent advances in the deep
learning of natural language processing seriously, and consider the resulting artificial neural language
models (NLMs) as plausible models of sentence processing. (p.8) 2) NLMs describe syntactic and linguistic processing at a much lower description level compared to common models in psycholinguistics. They could therefore be interpreted as a plausible implementation of the cognitive operations our brain generates during sentence comprehension (p.8) 3) The sparsity of the long-range agreement mechanism  [in the neural network]  thus provides a mechanistic explanation for capacity limitation in syntactic processing. (p. 11) 3) Several predictions in humans can be derived from the neural language model, both with respect to behavior and cortical processing (p. 13) 4) we found that both humans and neural language models make more number-agreement errors on the embedded verb of center-embedded clauses compared to the main one (...) Both humans and neural networks make more agreement errors on the inner verb (‘chase’) compared to the outer one (‘runs’) (p.13) 5) The model therefore predicts similar agreement-error patterns in human behavior. We note that this error pattern is consistent with ’structural forgetting’, a phenomenon reported in humans (p.14) 6) the two types of units that emerged in the models during training suggest that a similar division may be observed in cortical processing. (p.14) 7) Consequently, the activity of a single unit in the model in response to a feature would map to a large number of spiking neurons in the brain, all responsive to the same feature. Taken together, a single unit in the NLM could therefore correspond to possibly more than 106 neurons in the brain. (p.14) 8) neural language models are shown to provide precise and testable predictions about both human behavior and its underlying cortical mechanisms, therefore serving as appealing models for both cognitive and mechanistic aspects of human linguistic performance (p.14-15) 9) Artificial neural networks may, as a first approximation, be useful guides to understand how syntactic information is encoded in the brain of human adults."	The authors engage in a thorough review of psycholinguistic literature regarding sentence processing and propose ANN as a more precise model to explain human cognitive mechanisms involved in sentence processing. Thus, the model are considered plausible implementations of cognitive mechanisms and valid mechanistic explanations of human sentence processing. Moreover, they draw a direct link between the model's architecture and the structure of the human brain. Therefore, this is a clear example of considering LMs a pure replica of human linguistic cognition.							
Lukas Galke, Yoav Ram, Limor Raviv	2023	arXiv.org	4	https://doi.org/10.48550/arXiv.2302.12239	Experimental	Language evolution	GPT-3.2 RNN	Do deep neural network models exhibit the same learning and generalization advantage when trained on more structured linguistic input as human adults? Specifically, we investigate whether the advantage of compositionality in language learning and language use carries over to artificial learning systems, while considering GPT-3.5 as a pre-trained large language model and a custom model architecture based on recurrent neural networks (RNNs) trained from scratch. 	They use ten input languages with different degree of compositionality in their structure and compare the degree of grammatical generalizations, sistematicity of the generalization to humans response.	"1) unlike humans, artificial neural networks do not seem to benefit from more compositional structure when
they are made to develop their own communication protocol, at least without dedicated intervention [52, 53, 54, 55]. Thus, this finding raises the question of whether systematic and compositional linguistic structure is helpful at all for deep neural networks, and to what extent does compositionality affect the memorization and generalization abilities of deep neural networks learning a new language.(p.2) 2)  Our work contributes to the understanding of deep neural networks and large language models, sheds new light on the similarity between humans and machines, and, consequently, opens up future directions of simulating the very emergence of language and linguistic structure with deep neural network agents. (p.4) 3) All three learning systems (Humans, RNNs, and GPT-3) show the same trend: more compositionality in the input language leads to more systematic generalization. (p.10) 4) our findings strengthen the idea that language models are useful for studying human cognitive mechanisms, complementing the increasing evidence of similarity in language learning between humans and machines (p.11) 5) our results predict that children would also benefit from more systematic compositional structure in the same way adults do – a prediction we are currently testing (p. 11) 6) showing that linguistic structure is crucial for language learnability and
systematic generalization in neural networks. Our results suggest that more structured languages are easier to learn, regardless of the learning system: a human, a recurrent neural network, or a large language model. Thus, generalization capabilities are heavily influenced by compositional structure, with both biological and artificial learning systems benefiting from more structured input by facilitating more systematic and transparent
generalizations. (). 7) Our findings have further implications for machine learning, where systematic generalization beyond the training distribution is of high interest [16, 18, 75, 19, 20]. Specifically,
we show that seeding a learning system with well-structured inputs can improve their
ability to systematically generalize to new inputs (p.11) .QUOTE FROM GALKE & RAVIV, 2024, P.12 Integrating these biases into large language models may very well lead to more cognitively plausible models for gaining new insights on how children acquire their first language."	The authors give a language evolution perspective about LLMs and RNNs finding evidence of better performance of the model (i.e. human-like) if language evolution research is taken into account in the training of the model. Moreover, the similar behavior of the models and humans shows that characteristics of the input (more structured language vs less structured languages) overall improve learnability of language. The findings are interepreted as supporting evidence of the possibility of learning directly from data given certain properties present in the data (more structure), thus it supports studies that are in opposition to the poverty of stimulus theory. Thus, models are treated as a testbed for theory (dis)proving. Being language evolution directly linked to language acquisition studies, employing LLMs and RNNs in a similar way to language acquisition studies help researchers understanding patterns of language emergence in humans by looking at what improves the models and what it doesn't. However, the way the authors approach the investigation seems more cautious in their premise and their conclusion, if compared to the studies review before. The main goal of the research seems to simply suggest an integration of knowledge gained from language evolution to improve the models. However, the more the models behave human-like the more they seem to legitimize the practice of drawing generalizations over humans based on these similarities. More presicely, it is interesting to observe how the main goal seems to investigate what are the properties of language that make it learnable in general, by testing both biological systems (humans) and artificial systems (RNNs and LLMs), but it is unclear what this learnability results should tell us about the underlying mechanisms of human acquisition. However, this does not seem to be the goal of the study. On the contrary, by studying the qualities of the input the authors seem to be more interested in the language as a system itself rather than in human cognition handling language, even though they borrow methods and theoretical assumptions from research framework that are trying to make predictions about human linguistic cognition by looking at the output of RNNs and LLMs (see intro of the paper). Thus, the study, even though it still displays some degree of ambiguity regarding the way the models are considered, seems to find a better use of the model, by investigating language as a separate, autonomous natural object, without drawing bald inferences about the human brain itself in relation to the models. On the other hand, it has to be noted that the real unclarity about the paper is its choice of using the models in the first place for the kind of questions they have. Indeed, they report that they are testing children to understand whether they also benefit from more structured input and this raised the question on whether using artificial learning systems makes sense when trying to understand whether humans are sensitive and benefit from certain property of language - a striclty human phenomenon as fare as we know. Thus, even though this approach seems to gain some insight, I wonder whether it might be redudant also considering that similar research is currently done directly on children (Sergey's research) to disentangle language acquisition, so connected to language evolution. On a final note, what is perhaps more interesting to highlight for our analysis is the conclusive remark from Galke and Raviv, 2024 (p.12) where the same circularity observed in Chowdhury and Zamparelli, 2018 seems at play: if the behavior does not match human behavior new models that ultimately match it are needed and only when they finally match it they can be regarded as cognitively plausible and, thus, good linguistic/cognitive theory. Moreover, being good explanations for human behavior apparently make them easy to be mistaken for replicas of humans that are tested to understand humans, as source of evidence for mechanisms underlying human behavior.							
Gašper Beguš, Maksymilian D?bkowski, Ryan rhodes	2023	arXiv.org	9	https://doi.org/10.48550/arXiv.2305.00948	Experimental (model prompting)	Syntax, phonology, semantics	GPT-4 GPT-3	"They investigate ""whether large language models can ever achieve a similar capacity [to human linguists] to reason from their “knowledge” about language to analyze their own grammar. In this paper, we present several case studies which suggest that the GPT architecture is moving in that direction."""	The authors prompt GPT-4 and GPT-3 with theoretical questions about syntactic structures in English (e.g. wh-movement), phonological processes in Korean and in an artificial grammar and then ask to construct a lambda calculus of English sentences. They compare the responses of GPT-3 and 4.	"1) This paper shows that testing complex metalinguistic abilities of large language models is now a possible line of inquiry and argues that the results of such tests can provide valuable insights into LLMs’ general metacognitive abilities. (p.2) 2) theoretical linguistics constitutes one approach to analyzing natural language, and since LLMs are trained primarily on language data, linguistic theory affords an opportunity to explore the extent of LLMs’ higher-order meta-cognitive abilities.(p. 2) 3) We argue that theoretical linguistic formalism presents the perfect testing ground for accessing the metalinguistic abilities of large language models (...) This line of research can be understood as behavioral interpretability of deep learning (p.3) 4) Large language models acquire linguistic competence from the surface statistics of their training data. Our goal is to understand whether this is a sufficient basis to
analyze language itself. (p.3). 5) Human linguists have arrived at a range of analytical tools that comprise the science of linguistics by reasoning about language structure from their own knowledge of their native languages (that is, from their mental grammar). It remains to be seen whether large language models can ever achieve a similar capacity to reason from their “knowledge” about language to analyze their own grammar. In this paper, we present several case studies which suggest that the GPT architecture is moving in that direction. (p.4) 6) Further research will investigate whether large language models are capable of innovative solutions that were not hypothesized by humans thus far. (p.4) 7)  This line of research has already yielded some further insights into the GPT-4’s ability to do explicit recursion, a unique property of human language (p. 25)."	"The authors conduct an analysis of the responses of Chat-GPT 4 comparing them with the ones of Chat GPT-3 to investigate the models' metalinguistic abilities. They argue that this type of investigation may help to shed some light in the meta cognitive abilities of these models. While it is unclear what the authors mean by ""cognitive"", this use seems to suggest the attribution of some human capacities to these models, as cognition is essentially a phenomenon partaining to humans, or more broadly to living, non-artificial entities. However, there might be a use of the term and concept of cognition of which I am not aware of. Along these lines, it is noteworthy the quote number 5, where the authors choose to write ""human linguists"", as there can be a linguist that is not human. This citation, together with the others reported seem to suggest another kind of cognition at play in large language models that can be of help in shaping new ideas about language that humans have not thought about before. Thus, in this specific case, it seems more that the authors are considering the models way more than a simple replica of human linguistic cognition. LLMs seem to be autonomous, creative parsers able to aid researchers' work not only with simple, non-creative tasks, but also with higher cognitive tasks, which can be tested as students are tested for the knowledge acquired during a univeersity course."							
																		
																		
																		
																		
																		
																		
																		
																		
																		
																		
																		
																		
																		
																		
																		
																		
																		
																		
																		
																		
